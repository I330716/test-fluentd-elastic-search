apiVersion: v1
data:
  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
      @log_level warn
      port 24224
      bind 0.0.0.0
    </source>

  output.conf: |-

    # <match raw.kubernetes.**>
    #   @id raw.kubernetes
    #   @type detect_exceptions
    #   @log_level warn
    #   remove_tag_prefix raw
    #   message log
    #   stream stream
    #   multiline_flush_interval 5
    #   max_bytes 500000
    #   max_lines 1000
    # </match>

    # Enriches records with Kubernetes metadata
    # <filter kubernetes.**>
    #   @type kubernetes_metadata
    #   @log_level warn
    #   cache_size 10000
    #   watch false
    #   cache_ttl -1
    # </filter>
    # # Remove unneeded fields
    # <filter kubernetes.**>
    #   @type record_transformer
    #   @log_level warn
    #   remove_keys $.kubernetes.master_url,$.kubernetes.namespace_id
    # </filter>

    <filter kubernetes.**>
      @type genhashvalue

      keys time, tag
      hash_type md5    # md5/sha1/sha256/sha512/mur128
      base64_enc true
      base91_enc false
      set_key _hash
      separator _
      inc_time_as_key true
      inc_tag_as_key true
    </filter>
    
    #Decide which is shoot record and which is not
    <match kubernetes.**>
      @id rewrite_tag_filter
      @type rewrite_tag_filter
      @log_level debug
      <rule>
        key $.kubernetes.namespace_name
        pattern ^(shoot.*)
        tag $1
      </rule>
      <rule>
        key $.kubernetes.namespace_name
        pattern ^shoot.*
        invert true
        tag non-shoot.${tag}
      </rule>
    </match>
    #Do not collect fluentd's own logs to avoid infinite loops.
    <match fluent.**>
      @type null
      @log_level warn
    </match>

    <match shoot**>
      @id elasticsearch_dynamic
      @type elasticsearch_dynamic
      @log_level debug
      include_tag_key true
      host elasticsearch-logging.${record['kubernetes']['namespace_name']}.svc
      port 9200
      logstash_format true
      id_key _hash
      reconnect_on_error true
      # gives time plugin to read bulk response from server
      request_timeout 15s
      <buffer tag>
        @type file
        #Limit the number of queued chunks
        queued_chunks_limit_size 4096
        # The number of threads of output plugins, which is used to write chunks in parallel
        flush_thread_count 100
        @include /etc/fluent/config.d/buffer.options
        path /gardener/flentd-buffers/kubernetes.shoot1.buffer
      </buffer>
      # avoiding backup of unrecoverble errors
      @include /etc/fluent/config.d/socondary.option
    </match>

    <match **>
      @id elasticsearch_static
      @type elasticsearch
      @log_level debug
      include_tag_key true
      host elasticsearch-logging.garden.svc
      port 9200
      logstash_format true
      id_key _hash
      reconnect_on_error true
      request_timeout 15s
      <buffer>
        @type file
        #Limit the number of queued chunks
        queued_chunks_limit_size 2048
        # The number of threads of output plugins, which is used to write chunks in parallel
        flush_thread_count 8
        @include /etc/fluent/config.d/buffer.options
        path /gardener/flentd-buffers/kubernetes.garden1.buffer
      </buffer>
       @include /etc/fluent/config.d/socondary.option
    </match>
  system.conf: |-
    <system>
      root_dir /gardener/flentd-buffers/
    </system>

  buffer.options: |-
    #chunks per 5s
    timekey 30
    # delay for flush
    timekey_wait 0
    # The max size of each chunks
    chunk_limit_size 2MB
    # he size limitation of this buffer plugin instance
    total_limit_size 3G
    # The percentage of chunk size threshold for flushing  
    chunk_full_threshold 0.9 
    # no compression
    compress text
    # flush/write all buffer chunks at shutdown
    flush_at_shutdown
    # flush/write chunks per specified time
    flush_interval 10s 
    # The sleep interval seconds of threads to wait next flush trial (when no chunks are waiting)
    flush_thread_interval 5.0
    # How output plugin behaves when its buffer queue is full
    overflow_action drop_oldest_chunk
    # The maximum seconds to retry to flush while failing, until plugin discards buffer chunks
    retry_timeout 5m
    # output plugin will retry periodically with fixed intervals (configured via retry_wait)
    retry_type periodic
    # Seconds to wait before next retry to flush
    retry_wait 10
    retry_randomize false
    flush_mode interval
    retry_max_times 30
    # this is only for debug. When flush take more time than this it will produce warning
    slow_flush_log_threshold 60

  socondary.option: |-
    <secondary>
      @type null
    </secondary>

kind: ConfigMap
metadata:
  labels:
    app: fluentd-es
    role: logging
  name: fluentd-es-config
  namespace: garden
