apiVersion: v1
data:
  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
      @log_level warn
      port 24224
      bind 0.0.0.0
    </source>

    @include /etc/fluent/config.d/feedback.input.option

  output.conf: |-

    # <match raw.kubernetes.**>
    #   @id raw.kubernetes
    #   @type detect_exceptions
    #   @log_level warn
    #   remove_tag_prefix raw
    #   message log
    #   stream stream
    #   multiline_flush_interval 5
    #   max_bytes 500000
    #   max_lines 1000
    # </match>

    # Enriches records with Kubernetes metadata
    # <filter kubernetes.**>
    #   @type kubernetes_metadata
    #   @log_level warn
    #   cache_size 10000
    #   watch false
    #   cache_ttl -1
    # </filter>
    # # Remove unneeded fields
    # <filter kubernetes.**>
    #   @type record_transformer
    #   @log_level warn
    #   remove_keys $.kubernetes.master_url,$.kubernetes.namespace_id
    # </filter>

    <filter kubernetes.**>
      @type genhashvalue

      keys time, tag
      hash_type md5    # md5/sha1/sha256/sha512/mur128
      base64_enc true
      base91_enc false
      set_key _hash
      separator _
      inc_time_as_key true
      inc_tag_as_key true
    </filter>
    
    #Decide which is shoot record and which is not
    <match kubernetes.**>
      @id rewrite_tag_filter
      @type rewrite_tag_filter
      @log_level debug
      <rule>
        key $.kubernetes.namespace_name
        pattern ^(shoot.*)
        tag $1
      </rule>
      <rule>
        key $.kubernetes.namespace_name
        pattern ^shoot.*
        invert true
        tag seed
        #tag non-shoot.${tag}
      </rule>
    </match>
    #Do not collect fluentd's own logs to avoid infinite loops.
    <match fluent.**>
      @type null
      @log_level warn
    </match>

    <match shoot**>
      @id elasticsearch_dynamic
      @type elasticsearch_dynamic
      @log_level warn
      include_tag_key true
      host elasticsearch-logging.${record['kubernetes']['namespace_name']}.svc
      port 9200
      logstash_format true
      #use _hash field for log ID
      id_key _hash
      reconnect_on_error true
      # gives time plugin to read bulk response from server
      request_timeout 15s
      # this is only for debug. When flush take more time than this it will produce warning
      slow_flush_log_threshold 60
      <buffer tag, time>
        @type file
        #Limit the number of queued chunks
        # queued_chunks_limit_size 4096 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        queued_chunks_limit_size 2048
        # The number of threads of output plugins, which is used to write chunks in parallel
        # flush_thread_count 50 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        flush_thread_count 25
        @include /etc/fluent/config.d/buffer.options
        path /gardener/flentd-buffers/kubernetes.shoot1.buffer
      </buffer>
      # avoiding backup of unrecoverble errors
      @include /etc/fluent/config.d/feedback.output.option
    </match>

    <match **>
      @id elasticsearch_static
      @type elasticsearch
      @log_level warn
      include_tag_key true
      host elasticsearch-logging.garden.svc
      port 9200
      logstash_format true
      id_key _hash
      reconnect_on_error true
      request_timeout 20s
      # this is only for debug. When flush take more time than this it will produce warning
      slow_flush_log_threshold 60
      <buffer tag, time>
        @type file
        #Limit the number of queued chunks
        # queued_chunks_limit_size 2048 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        queued_chunks_limit_size 1024
        # The number of threads of output plugins, which is used to write chunks in parallel
        # flush_thread_count 8 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        flush_thread_count 4    
        @include /etc/fluent/config.d/buffer.options
        path /gardener/flentd-buffers/kubernetes.garden1.buffer
      </buffer>
       @include /etc/fluent/config.d/feedback.output.option
    </match>

    @include /etc/fluent/config.d/feedback.process.option
  system.conf: |-
    <system>
      root_dir /gardener/flentd-buffers/
    </system>

  buffer.options: |-
    # The max size of each chunks
    chunk_limit_size 15MB
    #chunks per 30s
    timekey 30
    # delay for flush
    timekey_wait 0
    # he size limitation of this buffer plugin instance
    total_limit_size 3G
    # The percentage of chunk size threshold for flushing  
    chunk_full_threshold 0.9 
    # no compression
    compress text
    # flush/write all buffer chunks at shutdown
    flush_at_shutdown true
    # flush/write chunks per specified time
    flush_interval 10s 
    # The sleep interval seconds of threads to wait next flush trial (when no chunks are waiting)
    flush_thread_interval 5.0
    # How output plugin behaves when its buffer queue is full
    overflow_action drop_oldest_chunk
    # The maximum seconds to retry to flush while failing, until plugin discards buffer chunks
    retry_timeout 100
    # output plugin will retry periodically with fixed intervals (configured via retry_wait)
    retry_type periodic
    # Seconds to wait before next retry to flush
    retry_wait 15
    retry_randomize false
    flush_mode interval
    retry_max_times 10

  secondary.null.option: |-
    <secondary>
      @type null
    </secondary>

  feedback.input.option: |- 
    <source>
      @type forward
      @id feedback_input
      @log_level warn
      port 24225
      bind 127.0.0.1
      @label @FEEDBACK
    </source>

  feedback.output.option: |-
    <secondary>
      @type forward
      send_timeout 60s
      recover_wait 10s
      hard_timeout 60s

      <server>
        name feedback
        host 127.0.0.1
        port 24225
      </server>
    </secondary>

  feedback.process.option: |-
    <label @FEEDBACK>
 
      <match shoot**>
        @id elasticsearch_dynamic_feedback
        @type elasticsearch_dynamic
        @log_level warn
        include_tag_key true
        host elasticsearch-logging.${record['kubernetes']['namespace_name']}.svc
        port 9200
        logstash_format true
        #makes _hash field the id of the given log
        id_key _hash
        reconnect_on_error true
        # gives time plugin to read bulk response from server
        request_timeout 15s
        # this is only for debug. When flush take more time than this it will produce warning
        slow_flush_log_threshold 60
        <buffer tag,time>
          @type file
          #Limit the number of queued chunks
          #queued_chunks_limit_size 2048!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
          queued_chunks_limit_size 1024
          # The number of threads of output plugins, which is used to write chunks in parallel
          flush_thread_count 4
          @include /etc/fluent/config.d/feedback.buffer.options
          path /gardener/flentd-buffers/kubernetes.shoot.feedback.buffer
        </buffer>
        # avoiding backup of unrecoverble errors
        @include /etc/fluent/config.d/secondary.null.option
      </match>

      <match **>
        @id elasticsearch_static_feedback
        @type elasticsearch
        @log_level warn
        include_tag_key true
        host elasticsearch-logging.garden.svc
        port 9200
        logstash_format true
        id_key _hash
        reconnect_on_error true
        # gives time plugin to read bulk response from server
        request_timeout 15s
        # this is only for debug. When flush take more time than this it will produce warning
        slow_flush_log_threshold 60
        <buffer tag,time>
          @type file
          #Limit the number of queued chunks
          queued_chunks_limit_size 512
          # The number of threads of output plugins, which is used to write chunks in parallel
          flush_thread_count 2
          @include /etc/fluent/config.d/feedback.buffer.options
          path /gardener/flentd-buffers/kubernetes.garden.feedback.buffer
        </buffer>
        @include /etc/fluent/config.d/secondary.null.option
      </match>
    </label>

  feedback.buffer.options: |-
    #chunks per 6m
    timekey 360
    # delay for flush
    timekey_wait 200
    # The max size of each chunks
    chunk_limit_size 25MB
    # he size limitation of this buffer plugin instance
    total_limit_size 1G
    # The percentage of chunk size threshold for flushing  
    chunk_full_threshold 0.9 
    # no compression
    compress text
    # flush/write all buffer chunks at shutdown
    flush_at_shutdown true
    # flush/write chunks per specified time
    flush_interval 5s 
    # The sleep interval seconds of threads to wait next flush trial (when no chunks are waiting)
    flush_thread_interval 5.0
    # How output plugin behaves when its buffer queue is full
    overflow_action drop_oldest_chunk
    # output plugin will retry periodically with fixed intervals (configured via retry_wait)
    retry_type periodic
    # Seconds to wait before next retry to flush
    retry_wait 75s
    retry_randomize false
    flush_mode interval
    retry_max_times 2

kind: ConfigMap
metadata:
  labels:
    app: fluentd-es
    role: logging
  name: fluentd-es-config
  namespace: garden
